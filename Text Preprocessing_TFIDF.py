# -*- coding: utf-8 -*-
"""Copy of SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lsNEoDztfzMJGy7GLredXmKI1NX0FxhW
"""

from google.colab import drive
drive.mount('/content/drive')

pip install sastrawi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
nltk.download('stopwords')
import string
import re
import spacy
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

df = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/datasetfix.csv')
df

print('Ukuran Dataset :', df.shape)
print('Kolom :', df.columns)

kolom_hapus = ['reviewId', 'userName', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent','repliedAt', 'appVersion', 'at']
df = df.drop(kolom_hapus, axis=1)

df.head(10)

"""## Handling Missing value-Ignore tuple



"""

df.dropna(subset=['content'],inplace = True)

df.duplicated().sum

df.isnull().sum

df.shape

"""# Teks Peprocessing

tahapan pada teks preprocessing yakni :
- Tokenizing
- Clean & Case Folding
- Normalisasi
- Stopword
- Lemmatization

## Tokenizing serta Clean & Case Folding
"""

import pandas as pd
import re

# Fungsi untuk memisahkan tanda baca yang terhubung dengan kata
def separate_punctuation(text):
    # Define pattern to separate punctuation
    pattern = r'(?<=[A-Za-z])([.,!?])|([.,!?])(?=[A-Za-z])'
    separated_text = re.sub(pattern, r' \1\2 ', text)
    return separated_text

# Fungsi untuk membersihkan teks
def clean_text(df, text_field, new_text_field_name):
    # Lowercasing
    df[new_text_field_name] = df[text_field].apply(lambda x: ' '.join(x)).str.lower()
    # Menghapus tanda baca, mention, link, dan karakter khusus lainnya
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))
    # Menghapus angka
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))
    # Pisahkan tanda baca yang terhubung dengan kata
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: separate_punctuation(elem))
    # Tokenisasi kembali setelah pemisahan tanda baca
    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: x.split())
    return df

# Tokenisasi awal
df['text_token'] = df['content'].apply(lambda x: x.split())

# Pembersihan teks
df = clean_text(df, 'text_token', 'text_clean')

# Menampilkan DataFrame
df

"""## Normalisasi"""

import pandas as pd

# Membaca kamus bahasa dari file CSV
data_kamus = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Kamus Lexicon/kamus_normalization.csv')

# Fungsi untuk normalisasi
def normalisasi(teks, kamus):
    kalimat_final = []
    for kata in teks:
        kata_benar = kamus[kamus['Tidak Baku'] == kata]['Baku'].values
        if len(kata_benar) > 0:
            kalimat_final.append(kata_benar[0])
        else:
            kalimat_final.append(kata)
    return kalimat_final


# Normalisasi teks
df['content_norm'] = df['text_clean'].apply(lambda x: normalisasi(x, data_kamus))

df

"""## STOPWORD REMOVAL"""

# Daftar kata-kata stop words tambahan
more_stopwords = {
    'dar', 'hai', 'txffzhybv', 'bg', 'bot', 'yg', 'deh', 'ypdhl', 'tidak', 'nic', 'bos', 'hmmm', 'ky', 'yaa', 'mo', 'fb', 'laah', 'br', 'blg', 'da', 'x', 'jt', 'dan',
    'y', 'b', 't', 'yang', 'sj', 'faq', 'jsajan', 'aja', 'mis', 'mf', 'hmm', 'jii', 'issi', 'the', 'kok', 'ng', 'di', 'nih', 'lah', 'adm', 'nig', 'min', 'y', 'kak', 'k', 'va',
    'dong', 'ai', 'nya', 'e', 'tuh', 'nih', 'di' , 'min','ke', 'dgn', 'nya', 'jadi', 'ada', 'nya', 'ah', 'aamiin', 'hehehe', 'hhhh', 'hey', 'hmmm', 'hmm', 'ram', 'the', 'tfr', 'wkwk'
}

# Membuat daftar kata-kata stop words
stop_words_factory = StopWordRemoverFactory()
stop_words = stop_words_factory.get_stop_words()
stop_words = stop_words.extend(more_stopwords)

# Menginisialisasi StopWordRemover dengan daftar stop words yang diperbarui
stopword_remover = stop_words_factory.create_stop_word_remover()

# Fungsi untuk menghapus stop words dari teks
def remove_stopwords(text):
    if isinstance(text, list):
        text = ' '.join(text)
    return stopword_remover.remove(text)

#proses stopword
df['text_stopword'] = df['content_norm'].apply(remove_stopwords)

df

data = df

df = pd.DataFrame(data)

file_path = '/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/hasilstopword.csv'

# Simpan DataFrame ke dalam file CSV
df.to_csv(file_path, index=False)

"""## Lemmatisasi"""

nlp = spacy.load("en_core_web_sm")

from spacy.tokens import Token
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Inisialisasi Stemmer dari Sastrawi
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Fungsi lemmatization menggunakan Sastrawi
def lemmatize_indonesian(token):
    return stemmer.stem(token.text)

# Tambahkan ekstensi untuk Token spaCy
Token.set_extension('lemma_indonesian', getter=lemmatize_indonesian, force=True)

# Inisialisasi spaCy dengan model bahasa Indonesia
nlp = spacy.blank('id')

# Tambahkan pipeline tokenizer spaCy
def custom_tokenizer(nlp):
    return spacy.tokenizer.Tokenizer(nlp.vocab)

nlp.tokenizer = custom_tokenizer(nlp)

# Contoh teks dalam bahasa Indonesia (harus diubah sesuai dengan kebutuhan)
data = df['text_stopword']
df = pd.DataFrame(data)

# Lakukan lemmatization pada kolom 'text_stopword'
df['hasil_lemma'] = df['text_stopword'].apply(lambda x: ' '.join([token._.lemma_indonesian for token in nlp(x)]))

# Tampilkan dataframe hasil
df

df = df.rename(columns={'text_stopword' : 'text asli','hasil_lemma': 'text_prepro'})
df

data = df

df = pd.DataFrame(data)

file_path = '/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/hasilpreprofix.csv'

# Simpan DataFrame ke dalam file CSV
df.to_csv(file_path, index=False)

data = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/hasilpreprofix.csv')
data.head(10)

data

"""## Labelling"""

pip install vaderSentiment

import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

lexiconpos = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Kamus Lexicon/positifbersih.csv')
lexiconneg = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Kamus Lexicon/negatifbersih.csv')

# Inisialisasi Sentiment Intensity Analyzer
analyzer = SentimentIntensityAnalyzer()

# Fungsi untuk melakukan pelabelan sentimen menggunakan VADER
def label_sentiment_vader(text, lexiconpos, lexiconneg):
    # Mendapatkan compound score dari VADER untuk teks
    compound_score = analyzer.polarity_scores(text)['compound']

    # Mengambil kata-kata dalam teks
    words = text.split()

    # Inisialisasi nilai score
    score = 0

    # Tentukan skor sentimen berdasarkan kata-kata dalam teks
    for word in words:
        if word in lexiconpos['word'].values:
            score += lexiconpos[lexiconpos['word'] == word]['weight'].values[0]
        elif word in lexiconneg['word'].values:
            score += lexiconneg[lexiconneg['word'] == word]['weight'].values[0]

    # Tentukan label sentimen berdasarkan nilai skor
    polarity = 'Positif' if score >= 0 else 'Negatif'

    return score, polarity

# Terapkan fungsi analisis sentimen pada kolom 'text_prepro'
labels = df['text_prepro'].apply(label_sentiment_vader, lexiconpos=lexiconpos, lexiconneg=lexiconneg)

# Pisahkan hasil labels menjadi skor dan polaritas
compound_scores, polarities = zip(*labels)

# Tambahkan kolom baru untuk skor dan sentimen
df['compound scores'] = compound_scores
df['Sentimen'] = polarities

# Menampilkan 10 baris pertama dari DataFrame
df.head(10)

# Menghitung jumlah sentimen positif, negatif, dan total
# Menghitung jumlah sentimen positif, negatif, dan total
jumlah_positif = df[df['Sentimen'] == 'Positif'].shape[0]
jumlah_negatif = df[df['Sentimen'] == 'Negatif'].shape[0]
jumlah_total = df.shape[0]

# Menampilkan hasil
print(f"Jumlah sentimen positif: {jumlah_positif}")
print(f"Jumlah sentimen negatif: {jumlah_negatif}")
print(f"Jumlah sentimen total: {jumlah_total}")

df

data = df

data = pd.DataFrame(data)

file_path = '/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/labellingfix.csv'

# Simpan DataFrame ke dalam file CSV
df.to_csv(file_path, index=False)



data = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/labellingfix.csv')

data

"""# TFIDF

"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import normalize

# Memuat data
data = pd.read_csv('/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/labellingfix.csv')
column = "text_prepro"

# Mengganti nilai NaN dengan string kosong
data[column] = data[column].fillna('')

# Membuat objek CountVectorizer dan TfidfVectorizer
count_vectorizer = CountVectorizer()
tfidf_vectorizer = TfidfVectorizer()

# Transformasi teks dengan CountVectorizer
TF_vector = count_vectorizer.fit_transform(data[column])
normalized_tf_vector = normalize(TF_vector, norm='l1', axis=1)

# Transformasi teks dengan TfidfVectorizer
tfs = tfidf_vectorizer.fit_transform(data[column])
IDF_vector = tfidf_vectorizer.idf_

# Mengalikan matriks TF yang sudah dinormalisasi dengan IDF
tfidf_mat = normalized_tf_vector.multiply(IDF_vector).toarray()

# Mengubah hasil menjadi DataFrame
df_tfidf = pd.DataFrame(tfidf_mat, columns=tfidf_vectorizer.get_feature_names_out())

# Menampilkan DataFrame TF-IDF
df_tfidf

data = df_tfidf

data = pd.DataFrame(data)

file_path = '/content/drive/MyDrive/Skripsi/Program Skripsi/Hasil/tfidf.csv'

# Simpan DataFrame ke dalam file CSV
data.to_csv(file_path, index=False)

data